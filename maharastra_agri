Link: http://erp.msamb.com/online/fpc

code:
from selenium import webdriver
from selenium.webdriver.support.ui import WebDriverWait as wait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.keys import Keys
import time
from selenium.webdriver.support import ui
from selenium.webdriver.common.by import By
from selenium import webdriver
from selenium.webdriver.common.desired_capabilities import DesiredCapabilities
from selenium.webdriver.common.action_chains import ActionChains
from selenium.common.exceptions import TimeoutException

import sys
from selenium.webdriver.support.ui import Select
import csv
from selenium.webdriver.common.keys import Keys as keys
chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')
driver = webdriver.Chrome(executable_path=r'C:/Users/Acer/Downloads/chromedriver.exe')
wd = webdriver.Chrome(executable_path=r'C:/Users/Acer/Downloads/chromedriver.exe')

#select.select_by_visible_text('Akola'

from bs4 import BeautifulSoup
import time
import csv


places = ["Ahmednagar", "Akola","Amravati","Aurangabad","Beed","Bhandara","Chandrapur","Dhule","Gadchiroli","Gondiya","Hingoli","Jalgaon","Jalna","Kolhapur","Latur","Mumbai","NAVI MUMBAI","Nagpur","Nanded","Nandurbar","Nashik","Palghar","Parbhani","Pune","Raigad","Ratnagiri","Sangli","Satara","Sindhudurg","Solapur","Thane","Usmanabad","Wardha","Washim","Yavatmal"]
with open('Maharastra.csv', 'w', newline='',encoding = "utf-8") as csvfile:
    f = csv.writer(csvfile)
    for place in places: 
        wd.get('http://erp.msamb.com/online/fpc')
        district = wd.find_element_by_id('s2id_distABD')  
        district.find_element_by_tag_name('a').click()
        value = wd.find_element_by_id('s2id_autogen1_search')
        value.send_keys(place)
        value.send_keys(keys.ENTER)
        time.sleep(5)  
        soup = BeautifulSoup(wd.page_source, 'html.parser')
        header = []
        tags = soup.find_all("tr")
        heads = soup.find_all("th")
        head = []
        f.writerow(place)
        #print(head)  
        datah = []
        for tag in heads:                            
            datah.append(tag.text.strip())
        f.writerow(datah)
        print(datah)
        
        for tag in tags:        
            data = []
            cols = tag.find_all('td')  # find all td tags
            for td in cols:
                data.append(td.text.strip())
            f.writerow(data)
            print(data)